{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fca628a",
   "metadata": {
    "papermill": {
     "duration": 0.004195,
     "end_time": "2024-08-12T00:10:25.274899",
     "exception": false,
     "start_time": "2024-08-12T00:10:25.270704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summarize NDFD Element by HexBin\n",
    "This notebook summarizes National Weather Service's National Digitial Forecast Database (NDFD) datasets polygon shapefile that consists of hexagons that cover all of California.\n",
    "The 'input_dicts' list is a list of dictionaries that provide information on the NDFD GRIB files to download and process, the type of summary stastic that will be generated for each hexagon, and the item IDs for each output ArcGIS Online Feature Service that will be updated with the summary information. Addtional details on the input dictonaries is provided below.  \n",
    "\n",
    "To run this notebook you must:\n",
    "1. Run the notebook within python environment with both the ArcGIS for Python API and the ArcPy libaries installed (we are currently using ann ArcGIS Online Advanced Python Notebook)\n",
    "2. Create the output feature services by publishing out the hexagon shapefile for each NDFD element/variable and time period.  These feature service will need to include the 'grid_id' field from the hexagon shapefile along with a 'Value' field with a double data type.  \n",
    "3. Update the input parameters like the download folder, Zonal Tables FGDB path, and the input hexbin path.\n",
    "4. Update the input dictonaries to match the NDFD variables that you want to summaries, and the item ids to feature services you want to update.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9636d85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T00:10:25.281587Z",
     "iopub.status.busy": "2024-08-12T00:10:25.281287Z",
     "iopub.status.idle": "2024-08-12T00:10:36.364964Z",
     "shell.execute_reply": "2024-08-12T00:10:36.364249Z"
    },
    "papermill": {
     "duration": 11.089342,
     "end_time": "2024-08-12T00:10:36.366985",
     "exception": false,
     "start_time": "2024-08-12T00:10:25.277643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import arcpy\n",
    "from arcpy.sa import *\n",
    "from arcgis.features import GeoAccessor, GeoSeriesAccessor, FeatureSet\n",
    "from arcgis.gis import GIS\n",
    "import requests\n",
    "#from tqdm.notebook import tqdm\n",
    "import pytz\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "\n",
    "# Set Start Date/Time to Track how long the notebook takes to run\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f2ccd",
   "metadata": {},
   "source": [
    "### Update the Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b6b20d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T00:10:36.373560Z",
     "iopub.status.busy": "2024-08-12T00:10:36.373182Z",
     "iopub.status.idle": "2024-08-12T00:10:37.141092Z",
     "shell.execute_reply": "2024-08-12T00:10:37.140364Z"
    },
    "papermill": {
     "duration": 0.773117,
     "end_time": "2024-08-12T00:10:37.142751",
     "exception": false,
     "start_time": "2024-08-12T00:10:36.369634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input Parameters\n",
    "# Login with current GIS User\n",
    "gis = GIS(\"home\")\n",
    "\n",
    "# Allow ArcPy to Overwrite Outputs?\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "# Download Folder - Where should the GRIB Files be Stored?\n",
    "download_folder = r\"C:\\git\\ndfd_processing\\NDFD\"\n",
    "\n",
    "# What it might be on AGOL\n",
    "# download_folder = r\"/arcgis/home/weather_data/NDFD/\"\n",
    "\n",
    "# Set Up FGDB to Store Zonal Stats Tables\n",
    "# Check to see if FGDB Exists to Store Zonal Stats Tables, if not create it\n",
    "if arcpy.Exists(r\"C:\\git\\ndfd_processing\\TEMP\\ZonalTables.gdb\"):\n",
    "    print(\"Zonal Table Aready Exists, Skipping\")\n",
    "else:\n",
    "    arcpy.management.CreateFileGDB(r\"C:\\git\\ndfd_processing\\TEMP\", \"ZonalTables.gdb\")\n",
    "\n",
    "# What it might be on AGOL\n",
    "# Check to see if FGDB Exists to Store Zonal Stats Tables, if not create it\n",
    "#if arcpy.Exists(\"/arcgis/home/weather_data/ZonalTables.gdb\"):\n",
    "#    print(\"Zonal Table Aready Exists, Skipping\")\n",
    "#else:\n",
    "#    arcpy.management.CreateFileGDB(\"/arcgis/home/weather_data/\", \"ZonalTables.gdb\")\n",
    "\n",
    "\n",
    "# Input Hexagons - What HexBin Data Should we Use?    \n",
    "input_hexbin = r\"C:\\git\\ndfd_processing\\HEXBIN\\hexbin.shp\"\n",
    "\n",
    "# What it might be on AGOL \n",
    "# What HexBin Data Should we Use?    \n",
    "#input_hexbin = \"/arcgis/home/weather_data/hexbin.shp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b6f091",
   "metadata": {},
   "source": [
    "### Update Input Dictonaries\n",
    "Each dictonary in the 'input_dicts' list represents a new NDFD Data Element/Variable to Process, please take a look at the list below to see some examples. \n",
    "* *Name*: The name you want to associate with the NDFD Data Element/Variable you are Summarizing\n",
    "* *URL*: The URL to the downloadable GRIB file\n",
    "* *Variable*: The variable name that you want to summarize. Hint: There is typically only one variable included in the NDFD GRIB Datasets, just open the *.bin file in ArcGIS Pro and use any of hte multidimenisal raster tools to find its name.\n",
    "* *Output Layer*: The name of the temporary output raster layer that is used in the analysis\n",
    "* *Output Table*: The path and name of the zonal statics as table tool output that is used in the analysis.\n",
    "* *Item IDs*: A dictonary that lists all the days/time periods you want to summarize and the corrosponding item ids for the feature service that will be updated with this information.\n",
    "* *Summary Type*: The type of summary statistic that you want to use to aggergate the NDFD GRIB data to the hexagons (MIN, MAX, etc.)\n",
    "* *Conversion*: Optional element if you wish to apply a conversion to the data.  Current options include 'Kelvin to Fahrenheit' and 'Meters per Second to Miles per Hour'.\n",
    "* *Rounding*: Optional element if you wish to round the summary values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1292a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Parameters\n",
    "inputs_dicts = [{'Name':'Maximum Daily Temperature (NDFD)', \n",
    "                'URL':'https://tgftp.nws.noaa.gov/SL.us008001/ST.opnl/DF.gr2/DC.ndfd/AR.conus/VP.001-003/ds.temp.bin', \n",
    "                'Variable':'T@HTGL', \n",
    "                'Output Layer':'temp_layer', \n",
    "                'Output Table':'/arcgis/home/weather_data/ZonalTables.gdb/MaxTempTable', \n",
    "                'Item IDs': {'Today': '8714f080ce6c4dc39d2819ded9e519d1', \n",
    "                             'Tomorrow': 'f435e9a75e22471d978ec0fd80cd7be3', \n",
    "                             'Day After Tomorrow (Ends at 5pm)': '2d6d87c6dd694b9994d53264dcfdc15e', \n",
    "                             'Entire 3 Day Period': '503c812174534c1ca6ce2ad73809f269'}, \n",
    "                'Summary Type':'MAX', \n",
    "                'Conversion':'Kelvin to Fahrenheit', \n",
    "                 'Rounding':1},\n",
    "               {'Name':'Minimum Daily Relative Humidity (NDFD)', \n",
    "                'URL':'https://tgftp.nws.noaa.gov/SL.us008001/ST.opnl/DF.gr2/DC.ndfd/AR.conus/VP.001-003/ds.rhm.bin', \n",
    "                'Variable':'RH@HTGL', \n",
    "                'Output Layer':'minrh_layer', \n",
    "                'Output Table':'/arcgis/home/weather_data/ZonalTables.gdb/minrh_zonal_stats', \n",
    "                'Item IDs': {'Today': '4becbf9861314407b23a94a15f51e6e4', \n",
    "                             'Tomorrow': '09812c61da4d408386cdb814d4a68767', \n",
    "                             'Day After Tomorrow (Ends at 5pm)': '55ecdcc0f90d4bda966ccf6e1b66c5da',\n",
    "                             'Entire 3 Day Period': 'a8378aaae8bb4070a9192a2449c59ba2'}, \n",
    "                'Summary Type':'MIN', \n",
    "                'Rounding':1},\n",
    "              {'Name':'Maximum Daily Wind Speed (NDFD)', \n",
    "                'URL':'https://tgftp.nws.noaa.gov/SL.us008001/ST.opnl/DF.gr2/DC.ndfd/AR.conus/VP.001-003/ds.wspd.bin', \n",
    "                'Variable':'WindSpd@HTGL', \n",
    "                'Output Layer':'windspeed_layer', \n",
    "                'Output Table':'/arcgis/home/weather_data/ZonalTables.gdb/windspeed_zonal_stats', \n",
    "                'Item IDs': {'Today': '769c8c14fc0947a6aa4e2ca7dfceb2ae', \n",
    "                             'Tomorrow': 'f92e185e15024979a386e554bdcb6e69', \n",
    "                             'Day After Tomorrow (Ends at 5pm)': '5dc72303989e4b1bb918e7b3e37a48fc', \n",
    "                             'Entire 3 Day Period': '1444cceb9fe04842922a30301b33256c'}, \n",
    "                'Summary Type':'MAX', \n",
    "               'Conversion':'Meters per Second to Miles per Hour', \n",
    "               'Rounding':1}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb915f4",
   "metadata": {},
   "source": [
    "### Run Process to Read and Summarize the NDFD Datasets by Hexagon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683c6a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T00:10:37.149371Z",
     "iopub.status.busy": "2024-08-12T00:10:37.149151Z",
     "iopub.status.idle": "2024-08-12T00:11:18.800486Z",
     "shell.execute_reply": "2024-08-12T00:11:18.799704Z"
    },
    "papermill": {
     "duration": 41.658334,
     "end_time": "2024-08-12T00:11:18.803827",
     "exception": false,
     "start_time": "2024-08-12T00:10:37.145493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Spatially-Enabled DataFrame of the Hexbins\n",
    "hex_sdf = pd.DataFrame.spatial.from_featureclass(input_hexbin)\n",
    "hex_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9096ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T00:11:18.811148Z",
     "iopub.status.busy": "2024-08-12T00:11:18.810929Z",
     "iopub.status.idle": "2024-08-12T00:11:18.863291Z",
     "shell.execute_reply": "2024-08-12T00:11:18.862167Z"
    },
    "papermill": {
     "duration": 0.058052,
     "end_time": "2024-08-12T00:11:18.864644",
     "exception": false,
     "start_time": "2024-08-12T00:11:18.806592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# Decorator to Check Run Time\n",
    "def check_runtime(func):\n",
    "    \"\"\"Function to check the runtime of other functions.\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        func_start = datetime.now()\n",
    "        result = func(*args, **kwargs)\n",
    "        func_end = datetime.now()\n",
    "        print(f\"Function {func.__name__} Took {round((func_end - func_start).seconds, 2)} Seconds to Run\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def kelvin2Fahrenheit(k):\n",
    "    \"\"\"Function to convert tempature from Kelvin to Fahrenheit\"\"\"\n",
    "    f = ((k-273.15)*(float(9)/float(5)))+32\n",
    "    return f\n",
    "\n",
    "def meters2mph(x):\n",
    "    \"\"\"Function to conver from Meters per Second and Miles per Hour\"\"\"\n",
    "    y = float(x)*2.23694\n",
    "    return y\n",
    "\n",
    "# Download File\n",
    "def downloadGRIB(input_url, download_folder):\n",
    "    \"\"\"Download the GRIB File\n",
    "        Parameters:\n",
    "            input_url (str): The URL of the GRIB File to Download\n",
    "            download_folder (str): The path to the folders that will contain the downloaded GRIB files\n",
    "        Returns:\n",
    "            str: The path to the downloaded GRIB file.\n",
    "    \"\"\"\n",
    "    output_file = os.path.basename(input_url)\n",
    "    grib_file = os.path.join(download_folder, output_file)\n",
    "    r = requests.get(input_url, allow_redirects=True)\n",
    "    open(grib_file, 'wb').write(r.content)\n",
    "    return grib_file\n",
    "\n",
    "# Create Raster Layer from GRIB\n",
    "def makeRasterLayer(input_file, output_layer, variable):\n",
    "    \"\"\"\n",
    "    Create a Raster Layer from a Multidimensional Raster File\n",
    "    Parameters:\n",
    "        input_file (str): The path to the GRIB or Other Multidimensional Raster File\n",
    "        output_layer (str): The name of the raster layer that will be created\n",
    "        variable (str): The variable that will be used to create the raster layer\n",
    "    Returns:\n",
    "        str: The name of the raster layer that was created\n",
    "    \"\"\"\n",
    "    arcpy.md.MakeMultidimensionalRasterLayer(input_file, output_layer, variable, \"ALL\")\n",
    "    return output_layer\n",
    "\n",
    "# Run Zonal Stats\n",
    "def runZonalStats(input_hexbin, output_layer, output_table, id_field=\"GRID_ID\"):\n",
    "    \"\"\"\n",
    "    Run Zonal Stastistics As Table using the hexbin polygons and the raster layer\n",
    "    Parameters:\n",
    "        input_hexbin (str): input hexbins???\n",
    "    \"\"\"\n",
    "    outZSaT = ZonalStatisticsAsTable(input_hexbin, id_field, output_layer, output_table, \"NODATA\", \"ALL\", \"ALL_SLICES\")\n",
    "    return output_table\n",
    "\n",
    "# Categorize Timestamp\n",
    "def categorizeStdTime(t):\n",
    "    \"\"\"Calculates the difference (in days)  between the analysis date adn the current date. Assigns the date a category.\"\"\"\n",
    "    #date_dif = (t.date() - datetime.now().date()).days\n",
    "    date_dif = (t.date() - datetime.now(pytz.timezone('America/Los_Angeles')).date()).days\n",
    "    if date_dif == 0:\n",
    "        category = 'Today'\n",
    "    elif date_dif == 1:\n",
    "        category = 'Tomorrow'\n",
    "    elif date_dif == 2:\n",
    "        category = 'Day After Tomorrow (Ends at 5pm)'\n",
    "    else:\n",
    "        category = 'Other/Unknown'\n",
    "    return category\n",
    "\n",
    "\n",
    "def summarizeTimePeriods(df, summary_type, all_periods_name, groupby_column = 'Time Period', time_periods=['Today', 'Tomorrow', 'Day After Tomorrow (Ends at 5pm)']):\n",
    "    output_dict = {}\n",
    "    summary_stat = summary_type.lower()\n",
    "    #time_periods = list(df[groupby_column].unique())\n",
    "    df = df[df['Time Period'].isin(time_periods)]\n",
    "    for t in time_periods:\n",
    "        temp_df = df[df['Time Period'] == t]\n",
    "        summary_df = temp_df.groupby('grid_id').agg({summary_type:summary_stat})\n",
    "        summary_df.reset_index(inplace=True)\n",
    "        # summary_hexbin = hexbin_sdf.merge(summary_df, on='grid_id', how='left')\n",
    "        # Clean Up Foramting\n",
    "        #rename_dict = {'grid_id':'GRID_ID', 'SHAPE':'SHAPE', summary_type:'Value'}\n",
    "        rename_dict = {'grid_id':'GRID_ID', summary_type:'New Value'}\n",
    "        summary_df.rename(columns=rename_dict, inplace=True)\n",
    "        summary_df = summary_df[list(rename_dict.values())]\n",
    "        # Add to Dictonary\n",
    "        output_dict[t] = summary_df\n",
    "        # Clean Up\n",
    "        del temp_df, summary_df #, summary_hexbin\n",
    "    # Search Across the Entire Time Period if all_periods_name is not none\n",
    "    if all_periods_name is not None:\n",
    "        summary_df = df.groupby('grid_id').agg({summary_type:summary_stat})\n",
    "        summary_df.reset_index(inplace=True)\n",
    "        #summary_hexbin = hexbin_sdf.merge(summary_df, on='grid_id', how='left')\n",
    "        rename_dict = {'grid_id':'GRID_ID', 'SHAPE':'SHAPE', summary_type:'Value'}\n",
    "        rename_dict = {'grid_id':'GRID_ID', summary_type:'New Value'}\n",
    "        summary_df.rename(columns=rename_dict, inplace=True)\n",
    "        summary_df = summary_df[list(rename_dict.values())]\n",
    "        output_dict[all_periods_name] = summary_df\n",
    "        del summary_df#, summary_hexbin\n",
    "    # Clean Up Formatting\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "\n",
    "def updateFeatureService(gis, sdf, agol_item_id, layer_num=0, chunkSize=100):\n",
    "    item = gis.content.get(agol_item_id)\n",
    "    layer = item.layers[layer_num]\n",
    "    \n",
    "    current_sdf = pd.DataFrame.spatial.from_layer(layer)\n",
    "    current_columns = current_sdf.columns.to_list()\n",
    "    # Columns to remove\n",
    "    removal_list = ['OBJECTID', 'Shape__Area', 'Shape__Length', 'CreationDate', 'Creator', 'EditDate', 'Editor']\n",
    "    current_columns = list(filter(lambda i: i not in removal_list, current_columns))\n",
    "    sdf = sdf[current_columns]\n",
    "    \n",
    "    print(f\"Checking Inputs to Compare SDF, Number of Records {sdf.shape[0]}\")\n",
    "    print(sdf.head())\n",
    "    \n",
    "    difference_sdf = compareLayer2SDF(layer, sdf, id_field = \"GRID_ID\", value_field=\"Value\")\n",
    "    \n",
    "    print(f\"Difference SDF, Number of Records {difference_sdf.shape[0]}\")\n",
    "    print(difference_sdf.head())\n",
    "    \n",
    "    updateResults = []\n",
    "    if difference_sdf is not None and len(difference_sdf)>0:\n",
    "        print(f\"Update Features {len(difference_sdf)}\")\n",
    "        numAddLoops = math.floor(len(difference_sdf)/chunkSize) + 1\n",
    "        startAddNum = 0\n",
    "        while numAddLoops > 0:\n",
    "            minAddNum = startAddNum\n",
    "            maxAddNum = startAddNum + chunkSize\n",
    "            startAddNum = startAddNum + chunkSize\n",
    "            numAddLoops = numAddLoops-1\n",
    "            temp_fs = difference_sdf[minAddNum:maxAddNum].copy()\n",
    "            update_featureset = FeatureSet.from_dataframe(temp_fs)\n",
    "            updateFeatures = layer.edit_features(updates=update_featureset)\n",
    "            updateResults.append(updateFeatures)\n",
    "    else:\n",
    "        print(\"No Records to Add\")\n",
    "    return updateResults\n",
    "\n",
    "def truncateLoadFeatureService(gis, sdf, agol_item_id, layer_num=0, chunkSize=100):\n",
    "    item = gis.content.get(agol_item_id)\n",
    "    layer = item.layers[layer_num]\n",
    "    \n",
    "    current_sdf = pd.DataFrame.spatial.from_layer(layer)\n",
    "    current_columns = current_sdf.columns.to_list()\n",
    "    # Columns to remove\n",
    "    removal_list = ['OBJECTID', 'Shape__Area', 'Shape__Length', 'CreationDate', 'Creator', 'EditDate', 'Editor']\n",
    "    current_columns = list(filter(lambda i: i not in removal_list, current_columns))\n",
    "    sdf = sdf[current_columns]\n",
    "\n",
    "    # Truncate\n",
    "    layer.manager.truncate()\n",
    "    \n",
    "    # Convert DataFrame to FeatureSet\n",
    "    addResults = []\n",
    "    if sdf is not None and len(sdf)>0:\n",
    "        print(f\"Add Features {len(sdf)}\")\n",
    "        numAddLoops = math.floor(len(sdf)/chunkSize) + 1\n",
    "        startAddNum = 0\n",
    "        while numAddLoops > 0:\n",
    "            minAddNum = startAddNum\n",
    "            maxAddNum = startAddNum + chunkSize\n",
    "            startAddNum = startAddNum + chunkSize\n",
    "            numAddLoops = numAddLoops-1\n",
    "            temp_fs = sdf[minAddNum:maxAddNum].copy()\n",
    "            adds_featureset = FeatureSet.from_dataframe(temp_fs)\n",
    "            addFeatures = layer.edit_features(adds=adds_featureset)\n",
    "            addResults.append(addFeatures)\n",
    "    else:\n",
    "        print(\"No Records to Add\")\n",
    "    return addResults\n",
    "\n",
    "\n",
    "def createFeatureService(gis, sdf, name, time_period, folder=\"Weather\"):\n",
    "    fs_name = f\"{name} - {time_period}\"\n",
    "    results = sdf.spatial.to_featurelayer(title=fs_name, gis=gis, folder=\"Weather\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def createFeatureServiceDict(gis, name, time_period_dict):\n",
    "    output_dict = {}\n",
    "    time_period_list = list(time_period_dict.keys())\n",
    "    for t in time_period_list:\n",
    "        period_sdf = time_period_dict[t]\n",
    "        new_service = createFeatureService(gis, period_sdf, name, t)\n",
    "        item_id = str(new_service.id)\n",
    "        output_dict[t] = item_id\n",
    "        del item_id, new_service, period_sdf\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "\n",
    "def compareLayer2SDF(layer, sdf, id_field = \"GRID_ID\", value_field=\"Value\"):\n",
    "    existing_sdf = pd.DataFrame.spatial.from_layer(layer)\n",
    "    print(f\"Existing SDF Number of Rows {existing_sdf.shape[0]}\")\n",
    "    print(f\"New SDF Number of Rows {sdf.shape[0]}\")\n",
    "    existing_sdf = existing_sdf[[id_field, value_field]]\n",
    "    print(\"Existing SDF\")\n",
    "    print(existing_sdf.head())\n",
    "    print(\"Dups in Existing SDF\")\n",
    "    print(existing_sdf[existing_sdf.duplicated([id_field], keep=False)])\n",
    "    print(\"Existing SDF After Set Index\")\n",
    "    existing_reindex = existing_sdf.set_index(id_field)\n",
    "    #existing_sdf.set_index(id_field, inplace=True)\n",
    "    print(existing_reindex.head())\n",
    "    #existing_dict = existing_sdf.set_index(id_field).to_dict(\"index\")\n",
    "    existing_dict = existing_reindex.to_dict(\"index\")\n",
    "    print(\"Existing Dictonary\")\n",
    "    #print(existing_dict)\n",
    "    # New Dictonary\n",
    "    temp_sdf = sdf[[id_field, value_field]]\n",
    "    new_dict = temp_sdf.set_index(id_field).to_dict(\"index\")\n",
    "    update_ids = []\n",
    "    update_dict = {}\n",
    "    for key in list(existing_dict.keys()):\n",
    "        old_value = existing_dict[key][value_field]\n",
    "        new_value = new_dict[key][value_field]\n",
    "        if old_value != new_value:\n",
    "            update_ids.append(key)\n",
    "            update_dict[key] = new_value\n",
    "    # Create Output SDF\n",
    "    output_sdf = existing_sdf[existing_sdf[id_field].isin(update_ids)].copy()\n",
    "    output_sdf[value_field] = output_sdf[id_field].apply(lambda x: update_dict[x])\n",
    "    print(f\"|-- Difference Detected {output_sdf.shape[0]} Rows\")\n",
    "    return output_sdf\n",
    "\n",
    "\n",
    "def compareDataFrames(input_dict, result, time_period, gis):\n",
    "    item_id = input_dict['Item IDs'][time_period]\n",
    "    \n",
    "    # Create a Spatially-Enabled DataFrame from the feature layer\n",
    "    item = gis.content.get(item_id)\n",
    "    layer = item.layers[0]\n",
    "    sdf = pd.DataFrame.spatial.from_layer(layer)\n",
    "    sdf.rename(columns={'Value':'Old Value'}, inplace=True)\n",
    "    \n",
    "    # Get the updated df\n",
    "    df = result[time_period]\n",
    "    \n",
    "    # Merge\n",
    "    updated_sdf = sdf.merge(df, how='left', left_on='grid_id', right_on = 'GRID_ID')\n",
    "    \n",
    "    # Calculate the Difference\n",
    "    updated_sdf['Difference in Values'] = updated_sdf.apply(lambda row: row['New Value'] - row['Old Value'], axis=1)\n",
    "    update_sdf = updated_sdf[updated_sdf['Difference in Values']!=0].copy()\n",
    "    update_sdf = update_sdf[['grid_id', 'GlobalID', 'OBJECTID', 'New Value']]\n",
    "    update_sdf.rename(columns={'New Value':'Value'}, inplace=True)\n",
    "    return update_sdf\n",
    "\n",
    "\n",
    "def updateFeatures(update_sdf, input_dict, result, time_period, gis, chunkSize=100):\n",
    "    \"\"\"\n",
    "    Update Features in a specific ArcGIS Online Feature Service\n",
    "    \"\"\"\n",
    "    item_id = input_dict['Item IDs'][time_period]\n",
    "    \n",
    "    # Create a Spatially-Enabled DataFrame from the feature layer\n",
    "    item = gis.content.get(item_id)\n",
    "    layer = item.layers[0]\n",
    "    \n",
    "    updateResults = []\n",
    "    \n",
    "    if update_sdf is not None and len(update_sdf)>0:\n",
    "        print(f\"Updating {len(update_sdf)} Features\")\n",
    "        numLoops = math.floor(len(update_sdf)/chunkSize)+1\n",
    "        startAddNum = 0\n",
    "        while numLoops > 0:\n",
    "            minAddNum = startAddNum\n",
    "            maxAddNum = startAddNum + chunkSize\n",
    "            startAddNum = startAddNum + chunkSize\n",
    "            numLoops = numLoops-1\n",
    "            temp_sdf = update_sdf[minAddNum:maxAddNum].copy()\n",
    "            update_fs = FeatureSet.from_dataframe(temp_sdf)\n",
    "            # Apply Update Edits to Service\n",
    "            try:\n",
    "                update_results = layer.edit_features(updates=update_fs)\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR TRYING TO UPDATE FEATURE SERVICE - {str(e)}\")\n",
    "                print(f\"Will Try Again in 5 Seconds\")\n",
    "                time.sleep(5.0)\n",
    "                try:\n",
    "                    update_results = layer.edit_features(updates=update_fs)\n",
    "                except Exception as e:\n",
    "                    print(f\"FAILED TO UPDATE A SECOND TIME\")\n",
    "                    print(f\"{str(e)}\")\n",
    "                    update_results = []                                              \n",
    "            updateResults.append(update_results)\n",
    "            del temp_sdf, update_fs\n",
    "    else:\n",
    "        print(\"No Records to Update\")\n",
    "    return updateResults\n",
    "\n",
    "\n",
    "\n",
    "def processGRIB(input_dict, input_hexbin, hex_sdf, all_periods = \"All Time Periods\", keep_intermediate_dfs=False):\n",
    "    \"\"\"\n",
    "    Process NDFD GRIB Files and Summarizes by HexBin\n",
    "    Parameters:\n",
    "        input_dict (dict): A dictonary that describes the input NDFD data, intermediate ouptuts, and feature services to be updated\n",
    "        input_hexbin (str): The path and name of the hexbin layer\n",
    "        hex_sdf (pd.DataFrame.spatial): A spatially enabable DataFrame that represents the hexbins\n",
    "        all_periods (str): The time category that is used to summarize all the relevant time periods\n",
    "        keep_intermeidate_dfs (bool): Flag to indiciate if the intermediate dataframes should be returned by this function\n",
    "    Returns:\n",
    "        Dictonary: A dictonary with one or two elements if keep_intermeidate_dfs. This dictonary includes the output DataFrames\n",
    "    \n",
    "    \"\"\"\n",
    "    if keep_intermediate_dfs:\n",
    "        intermediate_dict = {}\n",
    "    updates_dict = {}\n",
    "    name = input_dict['Name']\n",
    "    input_url = input_dict['URL']\n",
    "    output_layer = input_dict['Output Layer']\n",
    "    output_table = input_dict['Output Table']\n",
    "    #output_table = os.path.join(arcpy.env.scratchGDB, output_table_name)\n",
    "    print(f\"Output Table Check: {output_table}\")\n",
    "    \n",
    "    variable = input_dict['Variable']\n",
    "    summary_type = input_dict['Summary Type']\n",
    "    item_ids = input_dict['Item IDs']\n",
    "    if 'Conversion' in list(input_dict.keys()):\n",
    "        conversion_type = input_dict['Conversion']\n",
    "    else:\n",
    "        conversion_type = None\n",
    "    if 'Rounding' in list(input_dict.keys()):\n",
    "        rounding = int(input_dict['Rounding'])\n",
    "    else:\n",
    "        rounding = None\n",
    "    grib_file = downloadGRIB(input_url, download_folder)\n",
    "    raster_layer = makeRasterLayer(grib_file, output_layer, variable)\n",
    "    summary_table = runZonalStats(input_hexbin, raster_layer, output_table, id_field=\"GRID_ID\")\n",
    "    arcpy.management.Delete(raster_layer)\n",
    "    sdf = pd.DataFrame.spatial.from_table(summary_table)\n",
    "    sdf['Local Time'] = sdf['StdTime'].dt.tz_localize(\"UTC\").dt.tz_convert('America/Los_Angeles').dt.tz_localize(None)\n",
    "    #sdf['Local Time'] = sdf['StdTime']\n",
    "    sdf['Time Period'] = sdf['Local Time'].apply(lambda t: categorizeStdTime(t))\n",
    "    print(f\"Unique Time Periods\")\n",
    "    for tp in list(sdf['Time Period'].unique()):\n",
    "        print(f\"| -- {tp}\")\n",
    "    # Apply Conversions    \n",
    "    if conversion_type is not None:\n",
    "        if conversion_type == 'Kelvin to Fahrenheit':\n",
    "            sdf[summary_type] = sdf[summary_type].apply(lambda x: kelvin2Fahrenheit(x))\n",
    "        elif conversion_type == 'Meters per Second to Miles per Hour':\n",
    "            sdf[summary_type] = sdf[summary_type].apply(lambda x: meters2mph(x))\n",
    "        else:\n",
    "            print(\"ERROR - Unknown Conversion Type Specified, Please Try Again\")\n",
    "    if rounding is not None:\n",
    "        sdf[summary_type] = sdf[summary_type].apply(lambda x: round(float(x), rounding))\n",
    "    if keep_intermediate_dfs:\n",
    "        intermediate_dict[name] = sdf\n",
    "    sdf_dict = summarizeTimePeriods(sdf, summary_type, all_periods)\n",
    "    # Check to See if the Intermeddiate DataFrams should be included?\n",
    "    if keep_intermediate_dfs:\n",
    "        sdf_dict = {'Summary':sdf_dict, 'Intermediate':intermediate_dict}\n",
    "    return sdf_dict\n",
    "\n",
    "def createPlot(variable, intermediate_dict, results_dict, x, y, xlabel, ylabel, title, hexbin_id='FO-122', all_periods='Entire 3 Day Period', annotation_adj = 0.5):\n",
    "    \"\"\"\n",
    "    Create plot to visualize the NDFD Summary by hour for a single hexbin.\n",
    "    \"\"\"\n",
    "    # print(f\"Creating Plot for {variable}\")\n",
    "    # Intermediate/Source DataFrame\n",
    "    intermediate_df = intermediate_dict[variable][variable]\n",
    "    intermediate_df = intermediate_df[intermediate_df['grid_id'] == hexbin_id]\n",
    "    \n",
    "    # Set up Plot\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    #plt.style.use(\"dark_background\")\n",
    "    plt.figure(figsize=(24,6))\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Create Intial Line Plot\n",
    "    temp = sns.lineplot(data=intermediate_df, x=x, y=y)\n",
    "    temp.xaxis.set_major_locator(mdates.HourLocator(interval=1))\n",
    "    # set formatter\n",
    "    temp.xaxis.set_major_formatter(mdates.DateFormatter('%H'))\n",
    "    \n",
    "    # Lets go through each of the time periods in the results\n",
    "    time_periods = list(results_dict[variable].keys())\n",
    "    for t in time_periods:\n",
    "        if t != all_periods:\n",
    "            if t == 'Today':\n",
    "                minDate = datetime.now(pytz.timezone('America/Los_Angeles')).date()\n",
    "                maxDate = datetime.now(pytz.timezone('America/Los_Angeles')).date() + timedelta(days = 1)\n",
    "                temp.axvspan(xmin=minDate, xmax=maxDate, facecolor='white', alpha=0.5)\n",
    "                # Get Max for Today\n",
    "                temp_df = results_dict[variable][t]\n",
    "                if temp_df.shape[0] > 0:\n",
    "                    value = temp_df.loc[temp_df['GRID_ID'] == hexbin_id, 'New Value'].values[0]\n",
    "                    # Text Annotation\n",
    "                    xvalue = intermediate_df.loc[(intermediate_df['grid_id'] == hexbin_id) & (intermediate_df[y] == value) & (intermediate_df[x].dt.date == minDate), x].values\n",
    "                    try:\n",
    "                        temp.text(xvalue[0], value+annotation_adj, f'{minDate.strftime(\"%m/%d/%Y\")} {y.title()}: {str(value)}')\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error Adding Annotation {str(e)}\")\n",
    "                        print(f\"Potential Date/Times - {xvalue}\")\n",
    "                        print(f\"Max/Min Value - {value}\")\n",
    "                        print(f\"Checking Filters on Source DataFame\")\n",
    "                        print(f\"| --- Columns on intermediate df {intermediate_df.columns.to_list()}\")\n",
    "                        print(f\"| --- Number of Rows with Value Filter {intermediate_df[intermediate_df[y] == value].shape[0]}\")\n",
    "                        print(f\"| --- Number of Rows with grid_id Filter {intermediate_df[intermediate_df['grid_id'] == hexbin_id].shape[0]}\")\n",
    "                        print(f\"| --- | --- Rows with Value and Grid ID Filters {intermediate_df[(intermediate_df['grid_id'] == hexbin_id) & (intermediate_df[y] == value)].shape[0]}\")\n",
    "                        print(f\"| --- Number of Rows with Date Filter {intermediate_df[intermediate_df[x].dt.date == minDate].shape[0]}\")\n",
    "                        print(f\"| --- | --- Rows with Value, Grid ID, and Date Filters {intermediate_df[(intermediate_df['grid_id'] == hexbin_id) & (intermediate_df[y] == value) & (intermediate_df[x].dt.date == minDate)].shape[0]}\")\n",
    "                        print(f\"Looking at the potential dates\")\n",
    "                        print(f\"|-- {intermediate_df.loc[intermediate_df[y] == value, x].values}\")\n",
    "                        print(f\"|-- Min Date {minDate.strftime('%m/%d/%Y')}\")\n",
    "            elif t == '2 Days':\n",
    "                minDate = datetime.now(pytz.timezone('America/Los_Angeles')).date() + timedelta(days = 2)\n",
    "                maxDate = datetime.now(pytz.timezone('America/Los_Angeles')).date() + timedelta(days = 3)\n",
    "                temp.axvspan(xmin=minDate, xmax=maxDate, facecolor='white', alpha=0.5)\n",
    "                # Get Max for Today\n",
    "                temp_df = results_dict[variable][t]\n",
    "                if temp_df.shape[0] > 0:\n",
    "                    value = temp_df.loc[temp_df['GRID_ID'] == hexbin_id, 'New Value'].values[0]\n",
    "                    # Text Annotation\n",
    "                    # Text Annotation\n",
    "                    xvalue = intermediate_df.loc[(intermediate_df['grid_id'] == hexbin_id) & (intermediate_df[y] == value) & (intermediate_df[x].dt.date == minDate), x].values\n",
    "                    try:\n",
    "                        #xvalue = intermediate_df.loc[(intermediate_df['grid_id'] == hexbin_id) & (intermediate_df['MAX'] == value), 'Local Time'].values[0]\n",
    "                        temp.text(xvalue[0], value+annotation_adj, f'{minDate.strftime(\"%m/%d/%Y\")} {y.title()}: {str(value)}')\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error Adding Annotation {str(e)}\")\n",
    "                        print(f\"Potential Date/Times - {xvalue}\")\n",
    "                        print(f\"Max/Min Value - {value}\")\n",
    "                        print(f\"Checking Filters on Source DataFame\")\n",
    "                        print(f\"| --- Columns on intermediate df {intermediate_df.columns.to_list()}\")\n",
    "                        print(f\"| --- Number of Rows with Value Filter {intermediate_df[intermediate_df[y] == value].shape[0]}\")\n",
    "                        print(f\"| --- Number of Rows with grid_id Filter {intermediate_df[intermediate_df['grid_id'] == hexbin_id].shape[0]}\")\n",
    "                        print(f\"| --- | --- Rows with Value and Grid ID Filters {intermediate_df[(intermediate_df['grid_id'] == hexbin_id) & (intermediate_df[y] == value)].shape[0]}\")\n",
    "                        print(f\"| --- Number of Rows with Date Filter {intermediate_df[intermediate_df[x].dt.date == minDate].shape[0]}\")\n",
    "                        print(f\"| --- | --- Rows with Value, Grid ID, and Date Filters {intermediate_df[(intermediate_df['grid_id'] == hexbin_id) & (intermediate_df[y] == value) & (intermediate_df[x].dt.date == minDate)].shape[0]}\")\n",
    "                        print(f\"Looking at the potential dates\")\n",
    "                        print(f\"|-- {intermediate_df.loc[intermediate_df[y] == value, x].values}\")\n",
    "                        print(f\"|-- Min Date {minDate.strftime('%m/%d/%Y')}\")\n",
    "            else:\n",
    "                res = [int(i) for i in t.split() if i.isdigit()]\n",
    "                if len(res) > 0:\n",
    "                    day_adjust = res[0]\n",
    "                    minDate = datetime.now(pytz.timezone('America/Los_Angeles')).date() + timedelta(days = day_adjust)\n",
    "                    # Get Max for Today\n",
    "                    temp_df = results_dict[variable][t]\n",
    "                    if temp_df.shape[0] > 0:\n",
    "                        value = temp_df.loc[temp_df['GRID_ID'] == hexbin_id, 'New Value'].values[0]\n",
    "                        # Add Text\n",
    "                        xvalue = intermediate_df.loc[(intermediate_df['grid_id'] == hexbin_id) & (intermediate_df[y] == value) & (intermediate_df[x].dt.date == minDate), x].values\n",
    "                        try:\n",
    "                            temp.text(xvalue[0], value+annotation_adj, f'{minDate.strftime(\"%m/%d/%Y\")} {y.title()}: {str(value)}')\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error Adding Annotation {str(e)}\")\n",
    "                            print(f\"Potential Date/Times - {xvalue}\")\n",
    "                            print(f\"Max/Min Value - {value}\")\n",
    "                            print(f\"Checking Filters on Source DataFame\")\n",
    "                            print(f\"| --- Columns on intermediate df {intermediate_df.columns.to_list()}\")\n",
    "                            print(f\"| --- Number of Rows with Value Filter {intermediate_df[intermediate_df[y] == value].shape[0]}\")\n",
    "                            print(f\"| --- Number of Rows with grid_id Filter {intermediate_df[intermediate_df['grid_id'] == hexbin_id].shape[0]}\")\n",
    "                            print(f\"| --- | --- Rows with Value and Grid ID Filters {intermediate_df[(intermediate_df['grid_id'] == hexbin_id) & (intermediate_df[y] == value)].shape[0]}\")\n",
    "                            print(f\"| --- Number of Rows with Date Filter {intermediate_df[intermediate_df[x].dt.date == minDate].shape[0]}\")\n",
    "                            print(f\"| --- | --- Rows with Value, Grid ID, and Date Filters {intermediate_df[(intermediate_df['grid_id'] == hexbin_id) & (intermediate_df[y] == value) & (intermediate_df[x].dt.date == minDate)].shape[0]}\")\n",
    "                            print(f\"Looking at the potential dates\")\n",
    "                            print(f\"|-- {intermediate_df.loc[intermediate_df[y] == value, x].values}\")\n",
    "                            print(f\"|-- Min Date {minDate.strftime('%m/%d/%Y')}\")\n",
    "        else:\n",
    "            temp_df = results_dict[variable][t]\n",
    "            value = temp_df.loc[temp_df['GRID_ID'] == hexbin_id, 'New Value'].values[0]\n",
    "            # Add line to indicate max over time period\n",
    "            temp.axhline(value, ls='--', c='red')\n",
    "            temp.text(datetime.now(pytz.timezone('America/Los_Angeles')).date(), value+annotation_adj, f\"{y.title()} Value Over 3 Day Period: {str(value)}\", c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f4d2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T00:11:18.871200Z",
     "iopub.status.busy": "2024-08-12T00:11:18.870982Z",
     "iopub.status.idle": "2024-08-12T00:15:20.452697Z",
     "shell.execute_reply": "2024-08-12T00:15:20.451743Z"
    },
    "papermill": {
     "duration": 241.587116,
     "end_time": "2024-08-12T00:15:20.454419",
     "exception": false,
     "start_time": "2024-08-12T00:11:18.867303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dictonary to Store Results\n",
    "results_dict = {}\n",
    "# Dictonary to Store Intermediate DataFrames\n",
    "intermediate_dict = {}\n",
    "\n",
    "# Process GRIBs for Each Input\n",
    "for input_dict in inputs_dicts:\n",
    "    input_name = input_dict['Name']\n",
    "    print(f\"PROCESSING - {input_name}\")\n",
    "    result = processGRIB(input_dict, input_hexbin, hex_sdf, \"Entire 3 Day Period\", keep_intermediate_dfs=True)\n",
    "    # Add Results to Results Dictonary\n",
    "    results_dict[input_name] = result['Summary']\n",
    "    # Add Intermediate DFs to Intermediate Dictonary\n",
    "    intermediate_dict[input_name] = result['Intermediate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5356a9ba",
   "metadata": {
    "papermill": {
     "duration": 0.003121,
     "end_time": "2024-08-12T00:15:20.460662",
     "exception": false,
     "start_time": "2024-08-12T00:15:20.457541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create Some Plots for QA/QC - Forecast Time Series Plots for Downtown Sacramento (FO-122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62746887",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T00:15:20.467573Z",
     "iopub.status.busy": "2024-08-12T00:15:20.467338Z",
     "iopub.status.idle": "2024-08-12T00:15:22.046583Z",
     "shell.execute_reply": "2024-08-12T00:15:22.045917Z"
    },
    "papermill": {
     "duration": 1.584653,
     "end_time": "2024-08-12T00:15:22.048158",
     "exception": false,
     "start_time": "2024-08-12T00:15:20.463505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "createPlot('Maximum Daily Temperature (NDFD)', intermediate_dict, results_dict, x='Local Time', y='MAX', xlabel=\"Time of Day (Hours)\", ylabel=\"Maximum Hourly Temperature (Fahrenheit)\", title=\"NDFD Forecasted Maximum Temperature by Time of Day\", hexbin_id='FO-122', all_periods='Entire 3 Day Period', annotation_adj = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb42914",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T00:15:22.057923Z",
     "iopub.status.busy": "2024-08-12T00:15:22.057665Z",
     "iopub.status.idle": "2024-08-12T00:15:23.262081Z",
     "shell.execute_reply": "2024-08-12T00:15:23.261449Z"
    },
    "papermill": {
     "duration": 1.21129,
     "end_time": "2024-08-12T00:15:23.263713",
     "exception": false,
     "start_time": "2024-08-12T00:15:22.052423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "createPlot('Maximum Daily Wind Speed (NDFD)', intermediate_dict, results_dict, x='Local Time', y='MAX', xlabel=\"Time of Day (Hours)\", ylabel=\"Maximum Hourly Wind Speed (Miles Per Hour)\", title=\"NDFD Forecasted Maximum Wind Speed by Time of Day\", hexbin_id='FO-122', all_periods='Entire 3 Day Period', annotation_adj = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61093e84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T00:15:23.275310Z",
     "iopub.status.busy": "2024-08-12T00:15:23.275088Z",
     "iopub.status.idle": "2024-08-12T00:15:24.479297Z",
     "shell.execute_reply": "2024-08-12T00:15:24.478694Z"
    },
    "papermill": {
     "duration": 1.211826,
     "end_time": "2024-08-12T00:15:24.480770",
     "exception": false,
     "start_time": "2024-08-12T00:15:23.268944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "createPlot('Minimum Daily Relative Humidity (NDFD)', intermediate_dict, results_dict, x='Local Time', y='MIN', xlabel=\"Time of Day (Hours)\", ylabel=\"Minimum Hourly Relative Humidity (%)\", title=\"NDFD Forecasted Minimum Relative Humidity by Time of Day\", hexbin_id='FO-122', all_periods='Entire 3 Day Period', annotation_adj = -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83886248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T00:15:24.494530Z",
     "iopub.status.busy": "2024-08-12T00:15:24.494302Z",
     "iopub.status.idle": "2024-08-12T00:39:16.178020Z",
     "shell.execute_reply": "2024-08-12T00:39:16.177347Z"
    },
    "papermill": {
     "duration": 1431.692572,
     "end_time": "2024-08-12T00:39:16.179729",
     "exception": false,
     "start_time": "2024-08-12T00:15:24.487157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List to Store the Update Results\n",
    "updateResults = []\n",
    "\n",
    "# Process Each NDFD Element\n",
    "for input_dict in inputs_dicts:\n",
    "    input_name = input_dict['Name']\n",
    "    print(f\"Processing {input_name}\")\n",
    "    result = results_dict[input_name]\n",
    "    print(f\"Input has {len(result)} DataFrames\")\n",
    "    for time_period in list(input_dict['Item IDs'].keys()):\n",
    "        item_id = input_dict['Item IDs'][time_period]\n",
    "        update_sdf = compareDataFrames(input_dict, result, time_period, gis)\n",
    "        print(f\"|-- Time Period {time_period} has {len(update_sdf)} rows to update\")\n",
    "        update_results = updateFeatures(update_sdf, input_dict, result, time_period, gis)\n",
    "        del update_sdf, item_id\n",
    "        updateResults.append(update_results)\n",
    "    del result, input_name\n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136bd56a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T00:39:16.285271Z",
     "iopub.status.busy": "2024-08-12T00:39:16.284746Z",
     "iopub.status.idle": "2024-08-12T00:39:17.410920Z",
     "shell.execute_reply": "2024-08-12T00:39:17.410228Z"
    },
    "papermill": {
     "duration": 1.136084,
     "end_time": "2024-08-12T00:39:17.412299",
     "exception": false,
     "start_time": "2024-08-12T00:39:16.276215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete files in downlaod folder\n",
    "for f in os.listdir(download_folder):\n",
    "    delete_file = os.path.join(download_folder, f)\n",
    "    print(f\"Removing - {delete_file}\")\n",
    "    try:\n",
    "        os.remove(delete_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error Removing NDFD - {str(e)}\")\n",
    "\n",
    "# Delete FGDB\n",
    "try:\n",
    "    arcpy.Delete_management(\"/arcgis/home/weather_data/ZonalTables.gdb\")\n",
    "    print(\"Removed ZonalTables FGDB\")\n",
    "except Exception as e:\n",
    "    print(f\"Error Deleting FGDB - {str(e)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b9aab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T00:39:17.428930Z",
     "iopub.status.busy": "2024-08-12T00:39:17.428366Z",
     "iopub.status.idle": "2024-08-12T00:39:17.432401Z",
     "shell.execute_reply": "2024-08-12T00:39:17.431741Z"
    },
    "papermill": {
     "duration": 0.013669,
     "end_time": "2024-08-12T00:39:17.433692",
     "exception": false,
     "start_time": "2024-08-12T00:39:17.420023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Finished!\")\n",
    "end_time = datetime.now()\n",
    "print(f\"Process Complete - Took {round((end_time-start_time).seconds/60,2)} Minutes to Run\")"
   ]
  }
 ],
 "metadata": {
  "esriNotebookRuntime": {
   "notebookRuntimeName": "ArcGIS Notebook Python 3 Advanced",
   "notebookRuntimeVersion": "9.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1734.606097,
   "end_time": "2024-08-12T00:39:18.919591",
   "environment_variables": {},
   "exception": null,
   "input_path": "/tmp/arcgis/.tasks/0251f7603e3743b1a6f17c2e23d04c5b/cd6ebd167a8a4636bb40bb2063c04bff.ipynb",
   "output_path": "/tmp/arcgis/.tasks/0251f7603e3743b1a6f17c2e23d04c5b/output.ipynb",
   "start_time": "2024-08-12T00:10:24.313494",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
